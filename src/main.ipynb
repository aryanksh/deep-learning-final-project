{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4aecfb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Tweets\n",
    "\n",
    "Dataset used: [tweet_eval dataset (emotion subset)](https://huggingface.co/datasets/cardiffnlp/tweet_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3da0c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "765fe531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import html\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f49e937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b8c00",
   "metadata": {},
   "source": [
    "## Loading & Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf552f",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c33a0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = load_dataset(\"cardiffnlp/tweet_eval\", \"emotion\", split='train')\n",
    "ds_test = load_dataset(\"cardiffnlp/tweet_eval\", \"emotion\", split='test')\n",
    "ds_val = load_dataset(\"cardiffnlp/tweet_eval\", \"emotion\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a77c1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f964ba",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6ac18",
   "metadata": {},
   "source": [
    "preprocessing ideas:\n",
    "\n",
    "Replace common contractions (e.g., \"don't\" → \"do not\") to improve tokenization.\n",
    "\n",
    "Remove special characters and punctuation that don't contribute to emotion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6d15b",
   "metadata": {},
   "source": [
    "Normalizing all tweets to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d517d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(example):\n",
    "    example[\"text\"] = str(example[\"text\"]).lower()\n",
    "    return example\n",
    "\n",
    "ds_train = ds_train.map(lower_text)\n",
    "ds_test = ds_test.map(lower_text)\n",
    "ds_val = ds_val.map(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ce4b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocess(example):\n",
    "    text = example['text']\n",
    "    \n",
    "    new_text = []\n",
    "\n",
    "    # change all tags to users to \"@user\" and all links to \"http\"\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "\n",
    "    cleaned_text = \" \".join(new_text)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "\n",
    "    example['text'] = cleaned_text\n",
    "\n",
    "    return example\n",
    " \n",
    "ds_train = ds_train.map(apply_preprocess)\n",
    "ds_val = ds_val.map(apply_preprocess)\n",
    "ds_test = ds_test.map(apply_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e60af7",
   "metadata": {},
   "source": [
    "### Tokenize the features\n",
    "\n",
    "The label is already an integer, so only the text (the tweets themselves) needs to be tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d6f2f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "404a5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"google-bert/bert-base-uncased\"\n",
    "ROBERTA_MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, use_fast=False) # just used what hugging face docs had, we can change this tho\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(ROBERTA_MODEL, use_fast=False)\n",
    "\n",
    "def bert_tokenization(example): return bert_tokenizer(example['text'], padding='max_length', max_length=128, truncation=True)\n",
    "def roberta_tokenization(example): return roberta_tokenizer(example['text'], padding='max_length', max_length=128, truncation=True)\n",
    "\n",
    "# create copies of dataset for BERT and ROBERTA tokenization\n",
    "ds_train_bert = copy.deepcopy(ds_train)\n",
    "ds_test_bert = copy.deepcopy(ds_test)\n",
    "ds_val_bert = copy.deepcopy(ds_val)\n",
    "\n",
    "ds_train_roberta = copy.deepcopy(ds_train)\n",
    "ds_test_roberta = copy.deepcopy(ds_test)\n",
    "ds_val_roberta = copy.deepcopy(ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdca05",
   "metadata": {},
   "source": [
    "#### BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30a7e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_tokenized_bert = ds_train_bert.map(bert_tokenization, batched=True)\n",
    "ds_test_tokenized_bert = ds_test_bert.map(bert_tokenization, batched=True)\n",
    "ds_val_tokenized_bert = ds_val_bert.map(bert_tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1330d",
   "metadata": {},
   "source": [
    "Change format of BERT tokenized datasets into tensors, so that we can use PyTorch\n",
    "\n",
    "The `input_ids`, `token_type_ids`, and `attention_mask` columns will be the actual inputs to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc52daf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'torch',\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_tokenized_bert.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "ds_test_tokenized_bert.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "ds_val_tokenized_bert.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "\n",
    "ds_train_tokenized_bert.format # outputting some metadata of the tokenized training set, formatted for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9550b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in train: 3257\n",
      "Rows with None:      0\n"
     ]
    }
   ],
   "source": [
    "# 1. Define a filter to find None values\n",
    "def find_none(example):\n",
    "    return example['text'] is None\n",
    "\n",
    "# 2. Apply it to the training set\n",
    "bad_rows = ds_train.filter(find_none)\n",
    "\n",
    "print(f\"Total rows in train: {len(ds_train)}\")\n",
    "print(f\"Rows with None:      {len(bad_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7300a",
   "metadata": {},
   "source": [
    "#### ROBERTA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baecc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_tokenized_roberta = ds_train_roberta.map(roberta_tokenization, batched=True)\n",
    "ds_test_tokenized_roberta = ds_test_roberta.map(roberta_tokenization, batched=True)\n",
    "ds_val_tokenized_roberta = ds_val_roberta.map(roberta_tokenization, batched=True)\n",
    "\n",
    "# changing format of ROBERTA tokenized datasets into tensors for Pytorch\n",
    "ds_train_tokenized_roberta.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "ds_test_tokenized_roberta.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "ds_val_tokenized_roberta.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4ef0fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " TRAINING MODEL 1: BERT\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='612' max='612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [612/612 00:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.633448</td>\n",
       "      <td>0.786096</td>\n",
       "      <td>0.633162</td>\n",
       "      <td>0.746700</td>\n",
       "      <td>0.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.581300</td>\n",
       "      <td>0.791444</td>\n",
       "      <td>0.720166</td>\n",
       "      <td>0.738777</td>\n",
       "      <td>0.709041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.505600</td>\n",
       "      <td>0.823054</td>\n",
       "      <td>0.791444</td>\n",
       "      <td>0.723930</td>\n",
       "      <td>0.735187</td>\n",
       "      <td>0.718029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=612, training_loss=0.43825701327105754, metrics={'train_runtime': 39.244, 'train_samples_per_second': 248.981, 'train_steps_per_second': 15.595, 'total_flos': 642726071829504.0, 'train_loss': 0.43825701327105754, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" TRAINING MODEL 1: BERT\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "model_bert = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=4)\n",
    "\n",
    "args_bert = TrainingArguments(\n",
    "    output_dir=\"./results_bert\",\n",
    "    num_train_epochs=3,              # 3 loops is standard\n",
    "    per_device_train_batch_size=16,  # Reduce to 8 if you get CUDA OOM error\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"                 # Disable wandb logging to keep output clean\n",
    ")\n",
    "\n",
    "trainer_bert = Trainer(\n",
    "    model=model_bert,\n",
    "    args=args_bert,\n",
    "    train_dataset=ds_train_tokenized_bert,\n",
    "    eval_dataset=ds_val_tokenized_bert,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_bert.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vanilla Env",
   "language": "python",
   "name": "vanilla_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
