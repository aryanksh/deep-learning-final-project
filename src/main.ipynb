{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4aecfb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Tweets\n",
    "\n",
    "Dataset used: [tweet_eval dataset (emotion subset)](https://huggingface.co/datasets/cardiffnlp/tweet_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3da0c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "765fe531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import html\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f49e937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b8c00",
   "metadata": {},
   "source": [
    "## Loading & Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf552f",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c33a0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = load_dataset(\"cardiffnlp/tweet_eval\", \"emotion\", split='train')\n",
    "ds_test = load_dataset(\"cardiffnlp/tweet_eval\", \"emotion\", split='test')\n",
    "ds_val = load_dataset(\"cardiffnlp/tweet_eval\", \"emotion\", split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a77c1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f964ba",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6ac18",
   "metadata": {},
   "source": [
    "preprocessing ideas:\n",
    "\n",
    "Replace common contractions (e.g., \"don't\" → \"do not\") to improve tokenization.\n",
    "\n",
    "Remove special characters and punctuation that don't contribute to emotion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6d15b",
   "metadata": {},
   "source": [
    "Normalizing all tweets to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d517d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(example):\n",
    "    example[\"text\"] = str(example[\"text\"]).lower()\n",
    "    return example\n",
    "\n",
    "ds_train = ds_train.map(lower_text)\n",
    "ds_test = ds_test.map(lower_text)\n",
    "ds_val = ds_val.map(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce4b620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3257/3257 [00:00<00:00, 13093.37 examples/s]\n",
      "Map: 100%|██████████| 374/374 [00:00<00:00, 9562.20 examples/s]\n",
      "Map: 100%|██████████| 1421/1421 [00:00<00:00, 14625.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def apply_preprocess(example):\n",
    "    text = example['text']\n",
    "    \n",
    "    new_text = []\n",
    "\n",
    "    # change all tags to users to \"@user\" and all links to \"http\"\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "\n",
    "    cleaned_text = \" \".join(new_text)\n",
    "    cleaned_text = html.unescape(cleaned_text)\n",
    "\n",
    "    example['text'] = cleaned_text\n",
    "\n",
    "    return example\n",
    " \n",
    "ds_train = ds_train.map(apply_preprocess)\n",
    "ds_val = ds_val.map(apply_preprocess)\n",
    "ds_test = ds_test.map(apply_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e60af7",
   "metadata": {},
   "source": [
    "### Tokenize the features\n",
    "\n",
    "The label is already an integer, so only the text (the tweets themselves) needs to be tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d6f2f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "404a5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"google-bert/bert-base-uncased\"\n",
    "ROBERTA_MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, use_fast=False) # just used what hugging face docs had, we can change this tho\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(ROBERTA_MODEL, use_fast=False)\n",
    "\n",
    "def bert_tokenization(example): return bert_tokenizer(example['text'], padding='max_length', max_length=128, truncation=True)\n",
    "def roberta_tokenization(example): return roberta_tokenizer(example['text'], padding='max_length', max_length=128, truncation=True)\n",
    "\n",
    "# create copies of dataset for BERT and ROBERTA tokenization\n",
    "ds_train_bert = copy.deepcopy(ds_train)\n",
    "ds_test_bert = copy.deepcopy(ds_test)\n",
    "ds_val_bert = copy.deepcopy(ds_val)\n",
    "\n",
    "ds_train_roberta = copy.deepcopy(ds_train)\n",
    "ds_test_roberta = copy.deepcopy(ds_test)\n",
    "ds_val_roberta = copy.deepcopy(ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdca05",
   "metadata": {},
   "source": [
    "#### BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30a7e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_tokenized_bert = ds_train_bert.map(bert_tokenization, batched=True)\n",
    "ds_test_tokenized_bert = ds_test_bert.map(bert_tokenization, batched=True)\n",
    "ds_val_tokenized_bert = ds_val_bert.map(bert_tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1330d",
   "metadata": {},
   "source": [
    "Change format of BERT tokenized datasets into tensors, so that we can use PyTorch\n",
    "\n",
    "The `input_ids`, `token_type_ids`, and `attention_mask` columns will be the actual inputs to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc52daf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'torch',\n",
       " 'format_kwargs': {'device': device(type='cpu')},\n",
       " 'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_tokenized_bert.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'], device=device)\n",
    "ds_test_tokenized_bert.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'], device=device)\n",
    "ds_val_tokenized_bert.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'], device=device)\n",
    "\n",
    "ds_train_tokenized_bert.format # outputting some metadata of the tokenized training set, formatted for pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7300a",
   "metadata": {},
   "source": [
    "#### ROBERTA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baecc300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3257 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ds_train_tokenized_roberta \u001b[38;5;241m=\u001b[39m \u001b[43mds_train_roberta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroberta_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m ds_test_tokenized_roberta \u001b[38;5;241m=\u001b[39m ds_test_roberta\u001b[38;5;241m.\u001b[39mmap(roberta_tokenizer, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m ds_val_tokenized_roberta \u001b[38;5;241m=\u001b[39m ds_val_roberta\u001b[38;5;241m.\u001b[39mmap(roberta_tokenizer, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\aryan\\Downloads\\Fall 2025\\deep-learning\\project\\deep-learning-final-project\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aryan\\Downloads\\Fall 2025\\deep-learning\\project\\deep-learning-final-project\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3341\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[0;32m   3339\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3340\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[1;32m-> 3341\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munprocessed_kwargs):\n\u001b[0;32m   3342\u001b[0m                 check_if_shard_done(rank, done, content)\n\u001b[0;32m   3344\u001b[0m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aryan\\Downloads\\Fall 2025\\deep-learning\\project\\deep-learning-final-project\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3697\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[0;32m   3695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3696\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 3697\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[0;32m   3698\u001b[0m         num_examples_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[0;32m   3699\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[1;32mc:\\Users\\aryan\\Downloads\\Fall 2025\\deep-learning\\project\\deep-learning-final-project\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3647\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[1;34m(shard_iterable)\u001b[0m\n\u001b[0;32m   3645\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3647\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aryan\\Downloads\\Fall 2025\\deep-learning\\project\\deep-learning-final-project\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3570\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[1;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[0;32m   3568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[0;32m   3569\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m-> 3570\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "File \u001b[1;32mc:\\Users\\aryan\\Downloads\\Fall 2025\\deep-learning\\project\\deep-learning-final-project\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3073\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   3072\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 3073\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   3074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3075\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\aryan\\Downloads\\Fall 2025\\deep-learning\\project\\deep-learning-final-project\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3133\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 3133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `list[list[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3136\u001b[0m     )\n\u001b[0;32m   3138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   3139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `list[list[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3142\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "ds_train_tokenized_roberta = ds_train_roberta.map(roberta_tokenizer, batched=True)\n",
    "ds_test_tokenized_roberta = ds_test_roberta.map(roberta_tokenizer, batched=True)\n",
    "ds_val_tokenized_roberta = ds_val_roberta.map(roberta_tokenizer, batched=True)\n",
    "\n",
    "# changing format of ROBERTA tokenized datasets into tensors for Pytorch\n",
    "ds_train_tokenized_roberta.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'], device=device)\n",
    "ds_test_tokenized_roberta.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'], device=device)\n",
    "ds_val_tokenized_roberta.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "464d5d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column([\"“worry is a down payment on a problem you may never have'. \\xa0joyce meyer.  #motivation #leadership #worry\", \"my roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs\", \"no but that's so cute. atsu was probably shy about photos before but cherry helped her out uwu\", \"rooneys fucking untouchable isn't he? been fucking dreadful again, depay has looked decent(ish)tonight\", \"it's pretty depressing when u hit pan on ur favourite highlighter\"])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train_bert['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135d3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column([\"“worry is a down payment on a problem you may never have'. \\xa0joyce meyer.  #motivation #leadership #worry\", \"my roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs\", \"no but that's so cute. atsu was probably shy about photos before but cherry helped her out uwu\", \"rooneys fucking untouchable isn't he? been fucking dreadful again, depay has looked decent(ish)tonight\", \"it's pretty depressing when u hit pan on ur favourite highlighter\"])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
